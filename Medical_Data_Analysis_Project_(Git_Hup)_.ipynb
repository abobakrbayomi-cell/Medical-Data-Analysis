{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMkNK/eGuuhR0PpUvZaFF3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abobakrbayomi-cell/Medical-Data-Analysis/blob/main/Medical_Data_Analysis_Project_(Git_Hup)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InxCNwk77eh7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import io\n",
        "from google.colab import files\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Please upload the CSV file (Data.xlsx - Clean Data.csv)\")\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "glSY9ECe8QL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "if filename.endswith('.xlsx') or filename.endswith('.xls'):\n",
        "    df = pd.read_excel(io.BytesIO(uploaded[filename]), sheet_name='Clean Data')\n",
        "    print(\"\\n ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø´ÙŠØª (Clean Data) Ø¨Ù†Ø¬Ø§Ø­! .. ÙˆØ¯ÙŠ Ø¹ÙŠÙ†Ø© Ù…Ù†Ù‡:\")\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù„ÙŠ Ø±ÙØ¹ØªÙ‡ Ù…Ø´ Ø¥ÙƒØ³ÙŠÙ„! Ø­Ø§ÙˆÙ„ ØªØ±ÙØ¹ Ù…Ù„Ù Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯ Ø¨ØªØ§Ø¹Ù‡ .xlsx\")"
      ],
      "metadata": {
        "id": "hGHLZqZ99gy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discriptive Statistics**"
      ],
      "metadata": {
        "id": "3onfIBOLN-Fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¯Ø§ØªØ§\n",
        "try:\n",
        "    # ÙŠÙØ¶Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø¹Ù†Ø¯Ùƒ\n",
        "    df = pd.read_excel('Data.xlsx', sheet_name='Clean Data')\n",
        "\n",
        "    # ØªÙ†Ø¸ÙŠÙ Ø¹Ù…ÙˆØ¯ Group\n",
        "    df['Group'] = df['Group'].astype(str).str.replace('\"', '').str.strip()\n",
        "\n",
        "    # --- Ø§Ù„ØªØ¹Ø¯ÙŠÙ„: Ø®Ø±ÙŠØ·Ø© ØªØºÙŠÙŠØ± Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ---\n",
        "    group_map = {\n",
        "        'Group 1': 'Control',\n",
        "        'Group 2': 'Newly Diagnosed Patients',\n",
        "        'Group 3': 'Treated Patients'\n",
        "    }\n",
        "    df['Group'] = df['Group'].replace(group_map)\n",
        "    print(\"ØªÙ… ØªØ­Ø¯ÙŠØ« Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµØ­ÙŠØ­! Ø§Ù„Ø®Ø·Ø£: {e}\")\n",
        "\n",
        "# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© (ÙŠÙØ¶Ù„ ØªØ­Ø¯ÙŠØ¯Ù‡Ø§ Ø¨Ø§Ù„Ø§Ø³Ù… Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¯Ù‚Ø©)\n",
        "numeric_cols = ['HO-1', 'CP', 'CEA', 'Hb', 'Creatinine', 'ALT', 'Age', 'Cu', 'Fe', 'Zn', 'Ca']\n",
        "\n",
        "# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ù„Ø£Ø±Ù‚Ø§Ù…\n",
        "for col in numeric_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 2. Ø¯Ø§Ù„Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª (Ø´Ø§Ù…Ù„Ø© Q1, Q3, IQR)\n",
        "def calculate_detailed_stats(series):\n",
        "    if series.empty: return {}\n",
        "\n",
        "    mean = series.mean()\n",
        "    sem = series.sem()\n",
        "    sd = series.std()\n",
        "    minimum = series.min()\n",
        "    q1 = series.quantile(0.25)\n",
        "    median = series.median()\n",
        "    q3 = series.quantile(0.75)\n",
        "    maximum = series.max()\n",
        "    rng = maximum - minimum\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    return {\n",
        "        'Mean Â± SE': f\"{mean:.2f} Â± {sem:.2f}\",\n",
        "        'SD': round(sd, 2),\n",
        "        'Minimum': round(minimum, 2),\n",
        "        'Q1': round(q1, 2),\n",
        "        'Median': round(median, 2),\n",
        "        'Q3': round(q3, 2),\n",
        "        'Maximum': round(maximum, 2),\n",
        "        'Range': round(rng, 2),\n",
        "        'IQR': round(iqr, 2)\n",
        "    }\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£ÙˆÙ„: Total (ÙƒÙ„ Ø§Ù„Ø¯Ø§ØªØ§)\n",
        "# ---------------------------------------------------------\n",
        "total_rows = []\n",
        "for col in numeric_cols:\n",
        "    stats = calculate_detailed_stats(df[col].dropna())\n",
        "    row = {'Variable': col}\n",
        "    row.update(stats)\n",
        "    total_rows.append(row)\n",
        "\n",
        "df_total_final = pd.DataFrame(total_rows)\n",
        "\n",
        "# ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
        "cols_order_total = ['Variable', 'Mean Â± SE', 'SD', 'Minimum', 'Q1', 'Median', 'Q3', 'Maximum', 'Range', 'IQR']\n",
        "df_total_final = df_total_final[cols_order_total]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø«Ø§Ù†ÙŠ: Groups (ÙƒÙ„ Ù…ØªØºÙŠØ± ØªØ­ØªÙ‡ Ø§Ù„Ø¬Ø±ÙˆØ¨Ø§Øª)\n",
        "# ---------------------------------------------------------\n",
        "groups_rows = []\n",
        "# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø±ØºÙˆØ¨ Ù„Ù„Ø¹Ø±Ø¶\n",
        "desired_order = ['Control', 'Newly Diagnosed Patients', 'Treated Patients']\n",
        "\n",
        "for col in numeric_cols:\n",
        "    for group in desired_order:\n",
        "        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\n",
        "        if group in df['Group'].unique():\n",
        "            sub_series = df[df['Group'] == group][col].dropna()\n",
        "            stats = calculate_detailed_stats(sub_series)\n",
        "\n",
        "            row = {'Variable': col, 'Group': group}\n",
        "            row.update(stats)\n",
        "            groups_rows.append(row)\n",
        "\n",
        "df_groups_final = pd.DataFrame(groups_rows)\n",
        "\n",
        "# ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
        "cols_order_groups = ['Variable', 'Group', 'Mean Â± SE', 'SD', 'Minimum', 'Q1', 'Median', 'Q3', 'Maximum', 'Range', 'IQR']\n",
        "df_groups_final = df_groups_final[cols_order_groups]\n",
        "\n",
        "\n",
        "# 3. Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù…Ù„Ù Ø¥ÙƒØ³ÙŠÙ„\n",
        "output_filename = 'Descriptive_Statistics_Renamed.xlsx'\n",
        "with pd.ExcelWriter(output_filename) as writer:\n",
        "    df_total_final.to_excel(writer, sheet_name='Total', index=False)\n",
        "    df_groups_final.to_excel(writer, sheet_name='Groups Analysis', index=False)\n",
        "\n",
        "print(f\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­: {output_filename}\")"
      ],
      "metadata": {
        "id": "_X6dBUqV-TJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "from matplotlib.patches import Patch\n",
        "import numpy as np\n",
        "\n",
        "# Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù\n",
        "df = pd.read_excel('Data.xlsx', sheet_name='Clean Data')\n",
        "\n",
        "# The original 'try' block attempting to load a CSV has been removed\n",
        "# to resolve the IndentationError and ensure consistent Excel file loading.\n",
        "\n",
        "# 2. ØªÙ†Ø¸ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª\n",
        "df['Group'] = df['Group'].astype(str).str.replace('\"', '').str.strip()\n",
        "\n",
        "# 3. ØªØºÙŠÙŠØ± Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø­Ø³Ø¨ Ø§Ù„Ø·Ù„Ø¨\n",
        "group_map = {\n",
        "    'Group 1': 'Control',\n",
        "    'Group 2': 'Newly Diagnosed Patients',\n",
        "    'Group 3': 'Treated Patients'\n",
        "}\n",
        "df['Group'] = df['Group'].replace(group_map)\n",
        "\n",
        "# 4. ØªÙ†Ø¸ÙŠÙ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¬Ù†Ø³\n",
        "df['Sex'] = df['Sex'].astype(str).str.strip().str.upper()\n",
        "sex_map = {'M': 'Male', 'F': 'Female', 'MALE': 'Male', 'FEMALE': 'Female'}\n",
        "df['Sex'] = df['Sex'].replace(sex_map)\n",
        "\n",
        "# Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„ÙŠ Ù„ÙƒÙ„ Ø¬Ù†Ø³ (Ù„Ù„Ù€ Legend)\n",
        "sex_totals = df['Sex'].value_counts()\n",
        "\n",
        "# Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© 'Total' Ù„Ù„Ø¹ÙŠÙ†Ø© Ø§Ù„ÙƒÙ„ÙŠØ©\n",
        "df_total = df.copy()\n",
        "df_total['Group'] = 'Total'\n",
        "df_combined = pd.concat([df, df_total], ignore_index=True)\n",
        "\n",
        "# --- ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø±Ø³Ù… (Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø³Ø¨) ---\n",
        "# 1. Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ ÙƒÙ„ Ø¬Ù†Ø³ Ø¯Ø§Ø®Ù„ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø©\n",
        "df_counts = df_combined.groupby(['Group', 'Sex']).size().reset_index(name='Count')\n",
        "\n",
        "# 2. Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„ÙŠ Ù„ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© (Ù„Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„ÙŠÙ‡)\n",
        "group_counts_lookup = df_counts.groupby('Group')['Count'].sum()\n",
        "\n",
        "# 3. Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ©\n",
        "df_agg = df_counts.copy()\n",
        "df_agg['Percentage'] = df_agg.apply(lambda x: x['Count'] / group_counts_lookup[x['Group']], axis=1)\n",
        "\n",
        "# --- Ø§Ù„Ø±Ø³Ù… ---\n",
        "sns.set(style='whitegrid')\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø­ÙˆØ± Ø§Ù„Ø£ÙÙ‚ÙŠ\n",
        "group_order = ['Control', 'Newly Diagnosed Patients', 'Treated Patients', 'Total']\n",
        "\n",
        "ax = sns.barplot(\n",
        "    data=df_agg,\n",
        "    x='Group',\n",
        "    y='Percentage',\n",
        "    hue='Sex',\n",
        "    order=group_order,\n",
        "    palette='Greys', # Ø£Ù„ÙˆØ§Ù† Ø±Ù…Ø§Ø¯ÙŠØ©\n",
        "    edgecolor='black'\n",
        ")\n",
        "\n",
        "# ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø­ÙˆØ± Ø§Ù„ØµØ§Ø¯ÙŠ ÙƒÙ†Ø³Ø¨Ø© Ù…Ø¦ÙˆÙŠØ©\n",
        "ax.set_ylabel('Percentage', fontsize=14, weight='bold')\n",
        "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
        "ax.set_ylim(0, 1.15) # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ø¹Ù„ÙˆÙŠØ© Ù„Ù„ØªØ³Ù…ÙŠØ§Øª\n",
        "\n",
        "# --- Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ù†Ø³Ø¨ ÙÙˆÙ‚ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ---\n",
        "hue_order = sorted(df_agg['Sex'].unique()) # ['Female', 'Male']\n",
        "\n",
        "for i, container in enumerate(ax.containers):\n",
        "    hue_label = hue_order[i]\n",
        "    hue_data = df_agg[df_agg['Sex'] == hue_label]\n",
        "\n",
        "    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø±Ø³Ù…\n",
        "    hue_data = hue_data.set_index('Group').reindex(group_order)\n",
        "\n",
        "    labels = []\n",
        "    for group_name in group_order:\n",
        "        if group_name in hue_data.index and pd.notna(hue_data.loc[group_name, 'Count']):\n",
        "            count = int(hue_data.loc[group_name, 'Count'])\n",
        "            pct = hue_data.loc[group_name, 'Percentage']\n",
        "            labels.append(f\"{count} ({pct:.1%})\")\n",
        "        else:\n",
        "            labels.append(\"0\")\n",
        "\n",
        "    ax.bar_label(container, labels=labels, fontsize=10, color='black', padding=3, weight='bold')\n",
        "\n",
        "# --- ØªØ­Ø³ÙŠÙ† Ù…ÙØªØ§Ø­ Ø§Ù„Ø±Ø³Ù… (Legend) ---\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "new_handles = []\n",
        "new_labels = []\n",
        "\n",
        "for j, label_text in enumerate(labels):\n",
        "    total_count = sex_totals.get(label_text, 0)\n",
        "    new_label_text = f\"{label_text} (Total: {total_count})\"\n",
        "    new_labels.append(new_label_text)\n",
        "\n",
        "    # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£ØµÙ„ÙŠ\n",
        "    facecolor = handles[j].get_facecolor() if hasattr(handles[j], 'get_facecolor') else 'gray'\n",
        "\n",
        "    new_handles.append(\n",
        "        Patch(facecolor=facecolor, edgecolor='black', label=new_label_text)\n",
        "    )\n",
        "\n",
        "ax.legend(\n",
        "    handles=new_handles,\n",
        "    labels=new_labels,\n",
        "    title='Sex (with Total Count)',\n",
        "    loc='upper right',\n",
        "    fontsize=11,\n",
        "    title_fontsize=12\n",
        ")\n",
        "\n",
        "# Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\n",
        "ax.set_xlabel('Study Group', fontsize=14, weight='bold')\n",
        "plt.xticks(fontsize=11)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø©\n",
        "plt.savefig('sex_distribution_percentage_plot.png', dpi=300)\n",
        "print(\"Plot saved as 'sex_distribution_percentage_plot.png'\")"
      ],
      "metadata": {
        "id": "ai0YHuWRd-sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Normality (Shapiro-Wilk)**"
      ],
      "metadata": {
        "id": "H646TYoNkhLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df = pd.read_excel('Data.xlsx', sheet_name='Clean Data')\n",
        "\n",
        "    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¯Ù‚Ø©\n",
        "    df['Group'] = df['Group'].astype(str).str.strip().str.replace('\"', '').str.replace(\"'\", \"\")\n",
        "    print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.\")\n",
        "except:\n",
        "    print(\"âŒ Ø®Ø·Ø£: Ù…Ø´ Ù„Ø§Ù‚ÙŠ Ù…Ù„Ù Data.xlsx. ØªØ£ÙƒØ¯ Ø¥Ù†Ù‡ Ù…Ø±ÙÙˆØ¹.\")\n",
        "\n",
        "# 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ù„Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "results = []\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'Stage' in numeric_cols: numeric_cols.remove('Stage')\n",
        "\n",
        "groups = sorted(df['Group'].unique())\n",
        "\n",
        "print(\"\\nğŸ§ª Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ø§Ù„Ø§Ø¹ØªØ¯Ø§Ù„ÙŠØ© (Normality Test) Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª + Ø§Ù„ØªÙˆØªØ§Ù„...\\n\")\n",
        "\n",
        "for col in numeric_cols:\n",
        "    row = {'Variable': col}\n",
        "    is_group_normal = True # Ø¹Ø´Ø§Ù† Ù†Ø­Ø¯Ø¯ Ø§Ù„ØªÙˆØµÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø±ÙˆØ¨Ø§Øª\n",
        "\n",
        "    # --- Ø£ÙˆÙ„Ø§Ù‹: ÙØ­Øµ ÙƒÙ„ Ø¬Ø±ÙˆØ¨ Ù„ÙˆØ­Ø¯Ù‡ ---\n",
        "    for g in groups:\n",
        "        data = df[df['Group'] == g][col].dropna()\n",
        "\n",
        "        if len(data) >= 3:\n",
        "            try:\n",
        "                stat, p_value = stats.shapiro(data)\n",
        "                status = \"Normal\" if p_value > 0.05 else \"Not Normal\"\n",
        "\n",
        "                # Ù„Ùˆ Ø¬Ø±ÙˆØ¨ ÙˆØ§Ø­Ø¯ Ù…Ø´ Ø·Ø¨ÙŠØ¹ÙŠØŒ ÙŠØ¨Ù‚Ù‰ Ø§Ù„ØªÙˆØµÙŠØ© Non-Parametric\n",
        "                if p_value <= 0.05:\n",
        "                    is_group_normal = False\n",
        "\n",
        "                row[f'{g} (P-value)'] = f\"{p_value:.4f} ({status})\"\n",
        "            except:\n",
        "                row[f'{g} (P-value)'] = \"Error\"\n",
        "        else:\n",
        "            row[f'{g} (P-value)'] = \"N<3\"\n",
        "\n",
        "    # --- Ø«Ø§Ù†ÙŠØ§Ù‹: ÙØ­Øµ Ø§Ù„ØªÙˆØªØ§Ù„ (ÙƒÙ„ Ø§Ù„Ø¯Ø§ØªØ§ Ù…Ø¹ Ø¨Ø¹Ø¶) ---\n",
        "    total_data = df[col].dropna()\n",
        "    if len(total_data) >= 3:\n",
        "        try:\n",
        "            stat_t, p_val_t = stats.shapiro(total_data)\n",
        "            status_t = \"Normal\" if p_val_t > 0.05 else \"Not Normal\"\n",
        "            row['Total (P-value)'] = f\"{p_val_t:.4f} ({status_t})\"\n",
        "        except:\n",
        "            row['Total (P-value)'] = \"Error\"\n",
        "    else:\n",
        "        row['Total (P-value)'] = \"N<3\"\n",
        "\n",
        "    # --- Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ---\n",
        "    # (Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø±ÙˆØ¨Ø§Øª Ø¹Ø´Ø§Ù† Ù†Ø®ØªØ§Ø± Ø¨ÙŠÙ† ANOVA Ùˆ Kruskal)\n",
        "    if is_group_normal:\n",
        "        row['Suggestion'] = 'Parametric (ANOVA)'\n",
        "    else:\n",
        "        row['Suggestion'] = 'Non-Parametric (Kruskal-Wallis)'\n",
        "\n",
        "    results.append(row)\n",
        "\n",
        "# 3. Ø­ÙØ¸ ÙˆØ¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "normality_df = pd.DataFrame(results)\n",
        "\n",
        "# Ø¯Ø§Ù„Ø© ØªÙ„ÙˆÙŠÙ† Ø¹Ø´Ø§Ù† Ø§Ù„Ù…Ù„Ù ÙŠØ¨Ù‚Ù‰ Ø´ÙƒÙ„Ù‡ Ø­Ù„Ùˆ\n",
        "def highlight_cells(val):\n",
        "    val_str = str(val)\n",
        "    if 'Not Normal' in val_str:\n",
        "        return 'color: red; font-weight: bold'\n",
        "    elif 'Normal' in val_str:\n",
        "        return 'color: green'\n",
        "    return ''\n",
        "\n",
        "print(\"--- Ø¬Ø¯ÙˆÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ø®ØªØ¨Ø§Ø± Shapiro-Wilk (Groups & Total) ---\")\n",
        "display(normality_df.style.map(highlight_cells))\n",
        "\n",
        "# ØªØµØ¯ÙŠØ± Ù„Ù„Ø¥ÙƒØ³ÙŠÙ„\n",
        "output_file = 'Shapiro_Normality_With_Total.xlsx'\n",
        "normality_df.to_excel(output_file, index=False)\n",
        "print(f\"\\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¨Ø§Ø³Ù…: {output_file}\")"
      ],
      "metadata": {
        "id": "wLiuxfBI4blJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kruskal-Wallis**"
      ],
      "metadata": {
        "id": "TFKW3jvMSKV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø±ÙˆÙ (Post-hoc)\n",
        "!pip install scikit-posthocs\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import scikit_posthocs as sp\n",
        "\n",
        "# 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "try:\n",
        "    df = pd.read_excel('Data.xlsx', sheet_name='Clean Data')\n",
        "    # ØªÙ†Ø¸ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¬Ø±ÙˆØ¨Ø§Øª\n",
        "    df['Group'] = df['Group'].astype(str).str.strip().str.replace('\"', '').str.replace(\"'\", \"\")\n",
        "    print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.\")\n",
        "except:\n",
        "    print(\"âš ï¸ ØªØ£ÙƒØ¯ Ù…Ù† Ø±ÙØ¹ Ù…Ù„Ù Data.xlsx\")\n",
        "\n",
        "# 3. Ø¯Ø§Ù„Ø© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø±ÙˆÙ (Significance Letters)\n",
        "def get_letters(p_values, groups):\n",
        "    # Ø¯Ø§Ù„Ø© Ø°ÙƒÙŠØ© Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø­Ø±ÙˆÙ a, b, c Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª\n",
        "    g1, g2, g3 = groups[0], groups[1], groups[2]\n",
        "\n",
        "    try:\n",
        "        p12 = p_values.loc[g1, g2]\n",
        "        p13 = p_values.loc[g1, g3]\n",
        "        p23 = p_values.loc[g2, g3]\n",
        "    except:\n",
        "        return {g: 'a' for g in groups}\n",
        "\n",
        "    # Ù…ØµÙÙˆÙØ© Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ©\n",
        "    letters = {g1: '', g2: '', g3: ''}\n",
        "\n",
        "    # Ù„Ùˆ Ù…ÙÙŠØ´ Ø£ÙŠ ÙØ±ÙˆÙ‚\n",
        "    if p12 > 0.05 and p13 > 0.05 and p23 > 0.05:\n",
        "        return {g: 'a' for g in groups}\n",
        "\n",
        "    # Ù„Ùˆ ÙƒÙ„Ù‡ Ù…Ø®ØªÙ„Ù Ø¹Ù† Ø¨Ø¹Ø¶Ù‡\n",
        "    if p12 <= 0.05 and p13 <= 0.05 and p23 <= 0.05:\n",
        "        return {g1: 'a', g2: 'b', g3: 'c'}\n",
        "\n",
        "    # Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªØ¯Ø§Ø®Ù„\n",
        "    # 1 vs 2\n",
        "    if p12 > 0.05: # Ø²ÙŠ Ø¨Ø¹Ø¶\n",
        "        letters[g1] += 'a'\n",
        "        letters[g2] += 'a'\n",
        "    else: # Ù…Ø®ØªÙ„ÙÙŠÙ†\n",
        "        letters[g1] += 'a'\n",
        "        letters[g2] += 'b'\n",
        "\n",
        "    # 3 (Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ§Ù„ØªØ©)\n",
        "    # Ù†Ø´ÙˆÙ Ø¹Ù„Ø§Ù‚ØªÙ‡Ø§ Ø¨Ù€ 1\n",
        "    if p13 > 0.05: # Ø²ÙŠ 1\n",
        "        if 'a' in letters[g1]: letters[g3] += 'a'\n",
        "        else: letters[g3] += 'b' # (Ù†Ø§Ø¯Ø±Ø©)\n",
        "\n",
        "    # Ù†Ø´ÙˆÙ Ø¹Ù„Ø§Ù‚ØªÙ‡Ø§ Ø¨Ù€ 2\n",
        "    if p23 > 0.05: # Ø²ÙŠ 2\n",
        "        if 'b' in letters[g2] and 'b' not in letters[g3]: letters[g3] += 'b'\n",
        "        elif 'a' in letters[g2] and 'a' not in letters[g3]: letters[g3] += 'a'\n",
        "\n",
        "    # Ù„Ùˆ 3 Ù…Ø®ØªÙ„ÙØ© Ø¹Ù† Ø§Ù„Ø§ØªÙ†ÙŠÙ†\n",
        "    if p13 <= 0.05 and p23 <= 0.05:\n",
        "        letters[g3] = 'c'\n",
        "\n",
        "    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø­Ø±ÙˆÙ (ØªØ±ØªÙŠØ¨ Ø£Ø¨Ø¬Ø¯ÙŠ)\n",
        "    for g in letters:\n",
        "        letters[g] = \"\".join(sorted(set(letters[g])))\n",
        "        if letters[g] == \"\": letters[g] = \"c\" # fallback\n",
        "\n",
        "    return letters\n",
        "\n",
        "# 4. Ø§Ù„ØªØ¬Ù‡ÙŠØ² ÙˆØ§Ù„Ø­Ø³Ø§Ø¨\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'Stage' in numeric_cols: numeric_cols.remove('Stage')\n",
        "\n",
        "groups_order = sorted(df['Group'].unique())\n",
        "results = []\n",
        "\n",
        "print(\"\\nğŸš€ Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Kruskal-Wallis & Post-hoc Letters...\\n\")\n",
        "\n",
        "for col in numeric_cols:\n",
        "    # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¯Ø§ØªØ§\n",
        "    g1_data = df[df['Group'] == groups_order[0]][col].dropna()\n",
        "    g2_data = df[df['Group'] == groups_order[1]][col].dropna()\n",
        "    g3_data = df[df['Group'] == groups_order[2]][col].dropna()\n",
        "    all_data = [g1_data, g2_data, g3_data]\n",
        "\n",
        "    # Ø£) Levene's Test (Homogeneity)\n",
        "    try:\n",
        "        stat_lev, p_lev = stats.levene(*all_data)\n",
        "        lev_res = f\"{p_lev:.3f}\" + (\" *\" if p_lev < 0.05 else \"\")\n",
        "    except:\n",
        "        lev_res = \"NaN\"\n",
        "\n",
        "    # Ø¨) Kruskal-Wallis Test\n",
        "    try:\n",
        "        stat_k, p_k = stats.kruskal(*all_data)\n",
        "        k_res_stat = round(stat_k, 3)\n",
        "        k_res_p = f\"{p_k:.3f}\" + (\" *\" if p_k < 0.05 else \"\")\n",
        "    except:\n",
        "        k_res_stat, k_res_p = \"NaN\", \"NaN\"\n",
        "\n",
        "    # Ø¬) Post-hoc Letters (Dunn's Test)\n",
        "    letter_map = {g: 'a' for g in groups_order}\n",
        "    if p_k < 0.05: # Ù„Ùˆ ÙÙŠÙ‡ ÙØ±Ù‚ Ù…Ø¹Ù†ÙˆÙŠ Ù†Ø­Ø³Ø¨ Ø§Ù„Ø­Ø±ÙˆÙ\n",
        "        try:\n",
        "            ph_data = df[['Group', col]].dropna()\n",
        "            p_matrix = sp.posthoc_dunn(ph_data, val_col=col, group_col='Group', p_adjust='bonferroni')\n",
        "            letter_map = get_letters(p_matrix, groups_order)\n",
        "        except:\n",
        "            pass # Keep as 'a' if error\n",
        "\n",
        "    # Ø¯) ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØµÙ ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„\n",
        "    row = {'Variable': col}\n",
        "\n",
        "    for i, g_name in enumerate(groups_order):\n",
        "        data = all_data[i]\n",
        "        if data.empty:\n",
        "            row[f'Median Â± SIQR (\"{g_name}\")'] = \"-\"\n",
        "            continue\n",
        "\n",
        "        median = data.median()\n",
        "        q1 = data.quantile(0.25)\n",
        "        q3 = data.quantile(0.75)\n",
        "        siqr = (q3 - q1) / 2 # Ù…Ø¹Ø§Ø¯Ù„Ø© SIQR\n",
        "\n",
        "        ltr = letter_map[g_name]\n",
        "        # Ø§Ù„ØµÙŠØºØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: Median Â± SIQR (Letter)\n",
        "        row[f'Median Â± SIQR (\"{g_name}\")'] = f\"{median:.2f} Â± {siqr:.2f} ({ltr})\"\n",
        "\n",
        "    # Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
        "    row[\"Normality (Levene's test)\"] = lev_res\n",
        "    row['Kruskal_H_Statistic'] = k_res_stat\n",
        "    row['Kruskal_P_Value'] = k_res_p\n",
        "\n",
        "    results.append(row)\n",
        "\n",
        "# 5. Ø§Ù„Ø­ÙØ¸\n",
        "final_df = pd.DataFrame(results)\n",
        "filename = 'Kruskal_Analysis_Final.xlsx'\n",
        "final_df.to_excel(filename, index=False)\n",
        "\n",
        "print(f\"ğŸ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡! Ø§Ù„Ù…Ù„Ù Ø¬Ø§Ù‡Ø²: {filename}\")\n",
        "display(final_df.head())"
      ],
      "metadata": {
        "id": "JZOSkObKDJYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANOVA Test**"
      ],
      "metadata": {
        "id": "D3ZPWY0HSYXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import scikit_posthocs as sp\n",
        "import os\n",
        "\n",
        "# 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "filename = 'Data.xlsx'\n",
        "try:\n",
        "    if os.path.exists(filename):\n",
        "        df = pd.read_excel(filename, sheet_name='Clean Data')\n",
        "    else:\n",
        "        csv_files = [f for f in os.listdir('.') if 'clean' in f.lower() and f.endswith('.csv')]\n",
        "        if csv_files:\n",
        "            df = pd.read_csv(csv_files[0])\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯\")\n",
        "\n",
        "    df['Group'] = df['Group'].astype(str).str.strip().str.replace('\"', '').str.replace(\"'\", \"\")\n",
        "    print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Ø®Ø·Ø£: {e}\")\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "# 3. Ø¯Ø§Ù„Ø© ØªÙ†Ø³ÙŠÙ‚ P-value\n",
        "def format_p(p):\n",
        "    if pd.isna(p): return \"-\"\n",
        "    if p < 0.001: return \"< 0.001 *\"\n",
        "    sig = \" *\" if p < 0.05 else \"\"\n",
        "    return f\"{p:.3f}{sig}\"\n",
        "\n",
        "# 4. Ø¯Ø§Ù„Ø© Ø§Ù„Ø­Ø±ÙˆÙ (Ù†ÙØ³ Ø§Ù„Ù„ÙˆØ¬ÙŠÙƒ)\n",
        "def get_letters(p_values, groups):\n",
        "    g1, g2, g3 = groups[0], groups[1], groups[2]\n",
        "    letters = {g1: '', g2: '', g3: ''}\n",
        "    try:\n",
        "        p12, p13, p23 = p_values.loc[g1, g2], p_values.loc[g1, g3], p_values.loc[g2, g3]\n",
        "    except: return {g: 'a' for g in groups}\n",
        "\n",
        "    if p12 > 0.05 and p13 > 0.05 and p23 > 0.05: return {g: 'a' for g in groups}\n",
        "    if p12 <= 0.05 and p13 <= 0.05 and p23 <= 0.05: return {g1: 'a', g2: 'b', g3: 'c'}\n",
        "\n",
        "    # ØªØ¯Ø§Ø®Ù„Ø§Øª\n",
        "    if p12 > 0.05: letters[g1]+='a'; letters[g2]+='a'\n",
        "    else: letters[g1]+='a'; letters[g2]+='b'\n",
        "\n",
        "    match_1, match_2 = (p13 > 0.05), (p23 > 0.05)\n",
        "    if match_1: letters[g3] += ('a' if 'a' in letters[g1] else 'b')\n",
        "    if match_2:\n",
        "        if 'b' in letters[g2] and 'b' not in letters[g3]: letters[g3] += 'b'\n",
        "        elif 'a' in letters[g2] and 'a' not in letters[g3]: letters[g3] += 'a'\n",
        "\n",
        "    if not match_1 and not match_2: letters[g3] = 'c'\n",
        "    for g in letters:\n",
        "        cleaned = \"\".join(sorted(set(letters[g])))\n",
        "        letters[g] = cleaned if cleaned else \"c\"\n",
        "    return letters\n",
        "\n",
        "# 5. Ø§Ù„ØªÙ†ÙÙŠØ° (ANOVA + Tukey) Ø¨Ø¯ÙˆÙ† Ø£Ù‚ÙˆØ§Ø³\n",
        "if not df.empty:\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'Stage' in numeric_cols: numeric_cols.remove('Stage')\n",
        "    groups_order = sorted(df['Group'].unique())\n",
        "    results = []\n",
        "\n",
        "    print(\"\\nğŸš€ Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ ANOVA + Letters (Ø¨Ø¯ÙˆÙ† Ø£Ù‚ÙˆØ§Ø³)...\\n\")\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        g1_data = df[df['Group'] == groups_order[0]][col].dropna()\n",
        "        g2_data = df[df['Group'] == groups_order[1]][col].dropna()\n",
        "        g3_data = df[df['Group'] == groups_order[2]][col].dropna()\n",
        "        all_data = [g1_data, g2_data, g3_data]\n",
        "\n",
        "        # Levene\n",
        "        try:\n",
        "            stat_lev, p_lev = stats.levene(*all_data)\n",
        "            lev_res = format_p(p_lev)\n",
        "        except: lev_res = \"-\"\n",
        "\n",
        "        # ANOVA\n",
        "        try:\n",
        "            f_stat, p_anova = stats.f_oneway(*all_data)\n",
        "            f_res = f\"{f_stat:.2f}\"\n",
        "            p_res = format_p(p_anova)\n",
        "        except: f_res, p_res, p_anova = \"-\", \"-\", 1.0\n",
        "\n",
        "        # Tukey Letters\n",
        "        letter_map = {g: 'a' for g in groups_order}\n",
        "        if p_anova < 0.05:\n",
        "            try:\n",
        "                ph_data = df[['Group', col]].dropna()\n",
        "                p_matrix = sp.posthoc_tukey(ph_data, val_col=col, group_col='Group')\n",
        "                letter_map = get_letters(p_matrix, groups_order)\n",
        "            except: pass\n",
        "\n",
        "        # ØªØ¹Ø¨Ø¦Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„\n",
        "        row = {'Variable': col}\n",
        "        for i, g_name in enumerate(groups_order):\n",
        "            data = all_data[i]\n",
        "            if data.empty: row[f'Mean Â± SD (\"{g_name}\")'] = \"-\"; continue\n",
        "\n",
        "            mean_val = data.mean()\n",
        "            sd_val = data.std()\n",
        "\n",
        "            # Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§: Ø´ÙŠÙ„Ù†Ø§ Ø§Ù„Ø£Ù‚ÙˆØ§Ø³ ()\n",
        "            # Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù‡ØªØ¨Ù‚Ù‰: 12.5 Â± 2.1 a\n",
        "            row[f'Mean Â± SD (\"{g_name}\")'] = f\"{mean_val:.2f} Â± {sd_val:.2f} {letter_map[g_name]}\"\n",
        "\n",
        "        row[\"Normality (Levene's test)\"] = lev_res\n",
        "        row['F-ratio'] = f_res\n",
        "        row['P-value'] = p_res\n",
        "        results.append(row)\n",
        "\n",
        "    # Ø§Ù„Ø­ÙØ¸\n",
        "    anova_df = pd.DataFrame(results)\n",
        "    output_file = 'ANOVA_Results_No_Parentheses.xlsx'\n",
        "    anova_df.to_excel(output_file, index=False)\n",
        "\n",
        "    print(f\"ğŸ‰ ØªÙ… Ø§Ù„Ø­ÙØ¸ Ø¨Ù†Ø¬Ø§Ø­: {output_file}\")\n",
        "    display(anova_df.head())"
      ],
      "metadata": {
        "id": "SQVqrFjRSdyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bar Chart**"
      ],
      "metadata": {
        "id": "Wmx28yZayla0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bar chart with post hoc, Mean and SD**"
      ],
      "metadata": {
        "id": "g4B4nyzKOPws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "if 'anova_df' not in locals():\n",
        "    print(\"âš ï¸ Ø¬Ø¯ÙˆÙ„ anova_df Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯!\")\n",
        "else:\n",
        "    print(f\"ğŸš€ Ø¬Ø§Ø±ÙŠ Ø±Ø³Ù… {len(anova_df)} Ø¨Ø§Ø± Ø´Ø§Ø±Øª Ø¨Ø§Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„Ø¬Ø¯ÙŠØ¯ (Legend ÙÙˆÙ‚ ÙŠÙ…ÙŠÙ†)...\")\n",
        "\n",
        "    groups_labels_on_chart = ['Control', 'NDP', 'TP']\n",
        "    group_cols = [c for c in anova_df.columns if 'Mean Â± SD' in c]\n",
        "\n",
        "    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ù„ÙˆØ§Ù† ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø·\n",
        "    colors = ['#D3D3D3', '#A9A9A9', '#808080']\n",
        "    patterns = ['/', '\\\\', 'x']\n",
        "\n",
        "    # ØªØ¬Ù‡ÙŠØ² Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù€ Legend Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ø´Ø§Ù† Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙÙŠ ÙƒÙ„ Ø±Ø³Ù…Ø©\n",
        "    # Ø¯Ù‡ Ø¨ÙŠØ®Ù„ÙŠ Ø§Ù„Ø¨ÙˆÙƒØ³ ÙŠØ¸Ù‡Ø± ÙÙŠÙ‡ Ø´ÙƒÙ„ Ø§Ù„Ù…Ø±Ø¨Ø¹ Ø§Ù„ØµØºÙŠØ± ÙˆØ¬Ù†Ø¨Ù‡ Ø§Ù„ÙƒÙ„Ø§Ù…\n",
        "    legend_handles = [\n",
        "        mpatches.Patch(facecolor=colors[0], hatch=patterns[0], edgecolor='black', label='Control'),\n",
        "        mpatches.Patch(facecolor=colors[1], hatch=patterns[1], edgecolor='black', label='NDP: Newly Diagnosed Patients'),\n",
        "        mpatches.Patch(facecolor=colors[2], hatch=patterns[2], edgecolor='black', label='TP: Treated Patients')\n",
        "    ]\n",
        "\n",
        "    for index, row in anova_df.iterrows():\n",
        "        var_name = row['Variable']\n",
        "        print(f\"  - Drawing: {var_name}...\")\n",
        "\n",
        "        means = []\n",
        "        sds = []\n",
        "        letters = []\n",
        "\n",
        "        for col in group_cols:\n",
        "            val_str = str(row[col])\n",
        "            try:\n",
        "                parts = val_str.split()\n",
        "                means.append(float(parts[0]))\n",
        "                sds.append(float(parts[2]))\n",
        "                letters.append(parts[3])\n",
        "            except:\n",
        "                means.append(0)\n",
        "                sds.append(0)\n",
        "                letters.append('')\n",
        "\n",
        "        df_plot = pd.DataFrame({\n",
        "            'Group': groups_labels_on_chart,\n",
        "            'Mean': means,\n",
        "            'SD': sds,\n",
        "            'Letter': letters\n",
        "        })\n",
        "\n",
        "        # --- Ø§Ù„Ø±Ø³Ù… ---\n",
        "        fig, ax = plt.subplots(figsize=(10, 7)) # Ø±Ø¬Ø¹Ù†Ø§ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ù„Ø£Ù† Ø§Ù„Ù€ Legend Ø¨Ù‚Ù‰ Ø¬ÙˆØ©\n",
        "\n",
        "        bars = ax.bar(\n",
        "            df_plot['Group'],\n",
        "            df_plot['Mean'],\n",
        "            yerr=df_plot['SD'],\n",
        "            capsize=5,\n",
        "            color=colors,\n",
        "            edgecolor='black',\n",
        "            linewidth=1.0,\n",
        "            alpha=1.0\n",
        "        )\n",
        "\n",
        "        for bar, pattern in zip(bars, patterns):\n",
        "            bar.set_hatch(pattern)\n",
        "\n",
        "        ax.set_xlabel('Group', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel(f'Mean {var_name}', fontsize=12, fontweight='bold')\n",
        "\n",
        "        # ØªØ­Ø³ÙŠÙ† Ø´ÙƒÙ„ Ø§Ù„Ø´Ø¨ÙƒØ©\n",
        "        ax.grid(axis='y', linestyle=':', alpha=0.5, color='gray')\n",
        "        ax.set_axisbelow(True)\n",
        "\n",
        "        max_y = (df_plot['Mean'] + df_plot['SD']).max()\n",
        "        if pd.isna(max_y) or max_y == 0: max_y = 1\n",
        "        ax.set_ylim(0, max_y * 1.35)\n",
        "\n",
        "        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØµÙˆØµ (Mean + SD + Letters)\n",
        "        for i, bar in enumerate(bars):\n",
        "            r = df_plot.iloc[i]\n",
        "            height = bar.get_height()\n",
        "            if pd.isna(height): height = 0\n",
        "\n",
        "            label_text = f\"({r['Letter']})\\nMean: {r['Mean']}\\nSD: {r['SD']}\"\n",
        "\n",
        "            ax.text(\n",
        "                bar.get_x() + bar.get_width() / 2,\n",
        "                height + r['SD'] + (max_y * 0.02),\n",
        "                label_text,\n",
        "                ha='center',\n",
        "                va='bottom',\n",
        "                fontsize=10,\n",
        "                fontweight='bold',\n",
        "                color='black'\n",
        "            )\n",
        "\n",
        "        # --- Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù€ Legend (Ø§Ù„Ø¨ÙˆÙƒØ³ Ø§Ù„ØªØ¹Ø±ÙŠÙÙŠ) ---\n",
        "        # loc='upper right' : Ù…ÙƒØ§Ù† Ø§Ù„Ø¨ÙˆÙƒØ³\n",
        "        # prop={'size': 9} : Ø­Ø¬Ù… Ø§Ù„Ø®Ø· Ø£ØµØºØ±\n",
        "        # framealpha=0.9 : Ø´ÙØ§ÙÙŠØ© Ø®Ù„ÙÙŠØ© Ø§Ù„Ø¨ÙˆÙƒØ³\n",
        "        ax.legend(\n",
        "            handles=legend_handles,\n",
        "            loc='upper right',\n",
        "            fontsize=10,\n",
        "            frameon=True,\n",
        "            edgecolor='black',\n",
        "            framealpha=1,\n",
        "            fancybox=False # ÙŠØ®Ù„ÙŠ Ø§Ù„Ø­ÙˆØ§Ù Ù…Ø±Ø¨Ø¹Ø© Ù…Ø´ Ù…Ø¯ÙˆØ±Ø©\n",
        "        )\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        safe_var_name = \"\".join([c for c in var_name if c.isalnum() or c in (' ', '-', '_')]).strip()\n",
        "        plt.savefig(f'BarChart_{safe_var_name}.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    print(\"\\nâœ… ØªÙ… Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (Legend Ø§Ø­ØªØ±Ø§ÙÙŠ)!\")"
      ],
      "metadata": {
        "id": "Zt75hwQz1SAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Post Hoc**"
      ],
      "metadata": {
        "id": "p3HETOrxOIcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "var_name = \"CP\"\n",
        "\n",
        "if var_name not in anova_df['Variable'].values:\n",
        "    print(f\"âš ï¸ Ø§Ù„Ù…ØªØºÙŠØ± {var_name} Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯!\")\n",
        "else:\n",
        "    target_row = anova_df[anova_df['Variable'] == var_name].iloc[0]\n",
        "    print(f\"ğŸ“Š Ø¬Ø§Ø±ÙŠ Ø±Ø³Ù… {var_name}...\")\n",
        "\n",
        "    groups_labels_on_chart = ['Control', 'NDP', 'TP']\n",
        "    group_cols = [c for c in anova_df.columns if 'Mean Â± SD' in c]\n",
        "\n",
        "    means = []\n",
        "    sds = []\n",
        "    for col in group_cols:\n",
        "        val_str = str(target_row[col])\n",
        "        try:\n",
        "            parts = val_str.split()\n",
        "            means.append(float(parts[0]))\n",
        "            sds.append(float(parts[2]))\n",
        "        except:\n",
        "            means.append(0)\n",
        "            sds.append(0)\n",
        "\n",
        "    # Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø®ØµØµØ©\n",
        "    custom_letters = ['ab', 'a', 'b']\n",
        "\n",
        "    df_plot = pd.DataFrame({\n",
        "        'Group': groups_labels_on_chart,\n",
        "        'Mean': means,\n",
        "        'SD': sds,\n",
        "        'Letter': custom_letters\n",
        "    })\n",
        "\n",
        "    # --- Ø§Ù„Ø±Ø³Ù… ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "    colors = ['#D3D3D3', '#A9A9A9', '#808080']\n",
        "    patterns = ['/', '\\\\', 'x']\n",
        "\n",
        "    bars = ax.bar(\n",
        "        df_plot['Group'],\n",
        "        df_plot['Mean'],\n",
        "        yerr=df_plot['SD'],\n",
        "        capsize=5,\n",
        "        color=colors,\n",
        "        edgecolor='black',\n",
        "        linewidth=1.0,\n",
        "        alpha=1.0\n",
        "    )\n",
        "\n",
        "    for bar, pattern in zip(bars, patterns):\n",
        "        bar.set_hatch(pattern)\n",
        "\n",
        "    ax.set_xlabel('Group', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel(f'Mean {var_name}', fontsize=12, fontweight='bold')\n",
        "    ax.grid(axis='y', linestyle=':', alpha=0.5, color='gray')\n",
        "    ax.set_axisbelow(True)\n",
        "\n",
        "    max_y = (df_plot['Mean'] + df_plot['SD']).max()\n",
        "    if pd.isna(max_y) or max_y == 0: max_y = 1\n",
        "    ax.set_ylim(0, max_y * 1.35)\n",
        "\n",
        "    for i, bar in enumerate(bars):\n",
        "        row = df_plot.iloc[i]\n",
        "        height = bar.get_height()\n",
        "        if pd.isna(height): height = 0\n",
        "\n",
        "        label_text = f\"({row['Letter']})\\nMean: {row['Mean']}\\nSD: {row['SD']}\"\n",
        "\n",
        "        ax.text(\n",
        "            bar.get_x() + bar.get_width() / 2,\n",
        "            height + row['SD'] + (max_y * 0.02),\n",
        "            label_text,\n",
        "            ha='center',\n",
        "            va='bottom',\n",
        "            fontsize=10,\n",
        "            fontweight='bold',\n",
        "            color='black'\n",
        "        )\n",
        "\n",
        "    # --- Legend (Ø§Ù„Ø¨ÙˆÙƒØ³ Ø§Ù„Ø¬Ø¯ÙŠØ¯) ---\n",
        "    legend_handles = [\n",
        "        mpatches.Patch(facecolor=colors[0], hatch=patterns[0], edgecolor='black', label='Control'),\n",
        "        mpatches.Patch(facecolor=colors[1], hatch=patterns[1], edgecolor='black', label='NDP: Newly Diagnosed Patients'),\n",
        "        mpatches.Patch(facecolor=colors[2], hatch=patterns[2], edgecolor='black', label='TP: Treated Patients')\n",
        "    ]\n",
        "\n",
        "    ax.legend(\n",
        "        handles=legend_handles,\n",
        "        loc='upper right',\n",
        "        fontsize=10,\n",
        "        frameon=True,\n",
        "        edgecolor='black',\n",
        "        framealpha=1,\n",
        "        fancybox=False\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'BarChart_{var_name}_Final.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© CP Ø¨Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ!\")"
      ],
      "metadata": {
        "id": "LmEKVxl5Nc_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Box Blot**"
      ],
      "metadata": {
        "id": "f4LR17KDgsog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "try:\n",
        "    final_df = pd.read_excel('Kruskal_Analysis.xlsx')\n",
        "    print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ù†ØªØ§Ø¦Ø¬ Kruskal Ø¨Ù†Ø¬Ø§Ø­!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"âš ï¸ Ù…Ù„Ù 'Kruskal_Analysis.xlsx' Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯!\")\n",
        "    final_df = pd.DataFrame()\n",
        "\n",
        "# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ø£ØµÙ„ÙŠØ©\n",
        "if final_df.empty or 'df' not in locals():\n",
        "    if final_df.empty:\n",
        "        pass\n",
        "    else:\n",
        "        print(\"âš ï¸ Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ø£ØµÙ„ÙŠØ© 'df' Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©!\")\n",
        "else:\n",
        "    print(f\"ğŸš€ Ø¬Ø§Ø±ÙŠ Ø±Ø³Ù… {len(final_df)} Box Plot...\")\n",
        "\n",
        "    groups_labels_on_chart = ['Control', 'NDP', 'TP']\n",
        "    sorted_groups = sorted(df['Group'].unique())\n",
        "    result_cols = [c for c in final_df.columns if 'Median' in c and 'Group' in c]\n",
        "\n",
        "    colors = ['#D3D3D3', '#A9A9A9', '#808080']\n",
        "    patterns = ['/', '\\\\', 'x']\n",
        "\n",
        "    legend_handles = [\n",
        "        mpatches.Patch(facecolor=colors[0], hatch=patterns[0], edgecolor='black', label='Control'),\n",
        "        mpatches.Patch(facecolor=colors[1], hatch=patterns[1], edgecolor='black', label='NDP: Newly Diagnosed Patients'),\n",
        "        mpatches.Patch(facecolor=colors[2], hatch=patterns[2], edgecolor='black', label='TP: Treated Patients')\n",
        "    ]\n",
        "\n",
        "    for index, row in final_df.iterrows():\n",
        "        var_name = row['Variable']\n",
        "        if var_name not in df.columns:\n",
        "            continue\n",
        "\n",
        "        print(f\"  - Drawing: {var_name}...\")\n",
        "\n",
        "        data_to_plot = []\n",
        "        for g_name in sorted_groups:\n",
        "            vals = df[df['Group'] == g_name][var_name].dropna().values\n",
        "            data_to_plot.append(vals)\n",
        "\n",
        "        letters = []\n",
        "        for col in result_cols:\n",
        "            val_str = str(row[col])\n",
        "            try:\n",
        "                if '(' in val_str:\n",
        "                    letter = val_str.split('(')[-1].replace(')', '').strip()\n",
        "                else:\n",
        "                    letter = ''\n",
        "                letters.append(letter)\n",
        "            except:\n",
        "                letters.append('')\n",
        "\n",
        "        # --- Ø§Ù„Ø±Ø³Ù… ---\n",
        "        fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "        bplot = ax.boxplot(\n",
        "            data_to_plot,\n",
        "            patch_artist=True,\n",
        "            labels=groups_labels_on_chart,\n",
        "            widths=0.6,\n",
        "            flierprops={'marker': 'o', 'markerfacecolor': 'black', 'markersize': 5}\n",
        "        )\n",
        "\n",
        "        for patch, color, pattern in zip(bplot['boxes'], colors, patterns):\n",
        "            patch.set_facecolor(color)\n",
        "            patch.set_hatch(pattern)\n",
        "            patch.set_edgecolor('black')\n",
        "\n",
        "        for element in ['whiskers', 'caps', 'medians']:\n",
        "            plt.setp(bplot[element], color='black', linewidth=1.2)\n",
        "\n",
        "        ax.set_xlabel('Group', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel(f'Median {var_name}', fontsize=12, fontweight='bold')\n",
        "        ax.grid(axis='y', linestyle=':', alpha=0.5, color='gray')\n",
        "        ax.set_axisbelow(True)\n",
        "\n",
        "        # --- ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø±ÙˆÙ ÙÙŠ Ù…ÙƒØ§Ù† Ø«Ø§Ø¨Øª (ÙÙˆÙ‚ Ø§Ù„Ø´Ù†Ø¨ ÙÙ‚Ø·) ---\n",
        "\n",
        "        # Ø¨Ù†Ø­Ø³Ø¨ Ù…Ø³Ø§ÙØ© ØµØºÙŠØ±Ø© Ø«Ø§Ø¨ØªØ© (Offset) Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ù…Ø­ÙˆØ±\n",
        "        y_min_ax, y_max_ax = ax.get_ylim()\n",
        "        offset = (y_max_ax - y_min_ax) * 0.03 # Ù…Ø³Ø§ÙØ© 3% Ø«Ø§Ø¨ØªØ© ÙÙˆÙ‚ Ø§Ù„Ø´Ù†Ø¨\n",
        "\n",
        "        for i, letter in enumerate(letters):\n",
        "            if not letter: continue\n",
        "\n",
        "            # Ø§Ù„ÙƒÙˆØ¯ Ø¯Ù‡ Ø¨ÙŠØ¬ÙŠØ¨ Ù…ÙƒØ§Ù† \"Ø§Ù„ØºØ·Ø§Ø¡\" Ø§Ù„Ù„ÙŠ ÙÙˆÙ‚ Ø§Ù„Ø´Ù†Ø¨ Ø¨Ø§Ù„Ø¸Ø¨Ø·\n",
        "            # (i * 2) + 1 Ø¯ÙŠ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø«Ø§Ø¨ØªØ© ÙÙŠ matplotlib Ø¹Ø´Ø§Ù† ØªØ¬ÙŠØ¨ Ø§Ù„Ø´Ù†Ø¨ Ø§Ù„Ø¹Ù„ÙˆÙŠ\n",
        "            top_whisker_y = bplot['caps'][(i * 2) + 1].get_ydata()[0]\n",
        "\n",
        "            ax.text(\n",
        "                i + 1,\n",
        "                top_whisker_y + offset, # Ø§Ù„Ù…ÙƒØ§Ù†: ÙÙˆÙ‚ Ø§Ù„Ø´Ù†Ø¨ + Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ø«Ø§Ø¨ØªØ©\n",
        "                f\"({letter})\",\n",
        "                ha='center',\n",
        "                va='bottom',\n",
        "                fontsize=12,\n",
        "                fontweight='bold',\n",
        "                color='black'\n",
        "            )\n",
        "\n",
        "        ax.legend(\n",
        "            handles=legend_handles,\n",
        "            loc='upper right',\n",
        "            fontsize=10,\n",
        "            frameon=True,\n",
        "            edgecolor='black',\n",
        "            framealpha=1,\n",
        "            fancybox=False\n",
        "        )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        safe_var_name = \"\".join([c for c in var_name if c.isalnum() or c in (' ', '-', '_')]).strip()\n",
        "        plt.savefig(f'BoxPlot_KW_{safe_var_name}.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    print(\"\\nâœ… ØªÙ… Ø§Ù„Ø­ÙØ¸! Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¢Ù† ÙÙŠ Ù…ÙƒØ§Ù† Ø«Ø§Ø¨Øª ÙˆÙ…ÙˆØ­Ø¯ ÙÙˆÙ‚ Ø§Ù„Ù€ Whiskers.\")"
      ],
      "metadata": {
        "id": "CRdW1pFUaAzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# **Roc-Carve**"
      ],
      "metadata": {
        "id": "QrimBZAZrLCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "\n",
        "# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¯Ø§ØªØ§\n",
        "file_path = 'Data.xlsx'\n",
        "try:\n",
        "    df = pd.read_excel(file_path, sheet_name='Clean Data')\n",
        "except:\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "    except:\n",
        "        df = pd.DataFrame()\n",
        "\n",
        "if not df.empty:\n",
        "    if 'Group' in df.columns:\n",
        "        df['Group'] = df['Group'].astype(str).str.replace('\"', '').str.replace(\"'\", \"\").str.strip()\n",
        "\n",
        "    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª\n",
        "    exclude_cols = ['Group', 'Name', 'Sex', 'id', 'ID']\n",
        "    target_vars = [col for col in df.select_dtypes(include=[np.number]).columns if col not in exclude_cols]\n",
        "\n",
        "    # === Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ===\n",
        "    comparisons = [\n",
        "        ('Group 1', 'Group 2', 'Control vs NDP'),\n",
        "        ('Group 1', 'Group 3', 'Control vs TP'),\n",
        "        ('Group 3', 'Group 2', 'TP vs NDP')\n",
        "    ]\n",
        "\n",
        "    # Ø§Ù„ØªÙ…ÙŠÙŠØ² (Ø£Ù„ÙˆØ§Ù† + Ø£Ù†Ù…Ø§Ø· Ø®Ø·ÙˆØ· + Ø£Ø´ÙƒØ§Ù„ Ù‡Ù†Ø¯Ø³ÙŠØ©)\n",
        "    colors = ['#1f77b4', '#2ca02c', '#d62728']\n",
        "    linestyles = ['-', '--', '-.']\n",
        "    markers = ['o', '^', 's']\n",
        "\n",
        "    print(f\"Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª: {target_vars}\")\n",
        "\n",
        "    for target_var in target_vars:\n",
        "        try:\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plot_valid = False\n",
        "\n",
        "            for i, (neg_g, pos_g, label) in enumerate(comparisons):\n",
        "                sub_df = df[df['Group'].isin([neg_g, pos_g])].dropna(subset=[target_var])\n",
        "\n",
        "                if sub_df.empty: continue\n",
        "                plot_valid = True\n",
        "\n",
        "                y_true = (sub_df['Group'] == pos_g).astype(int)\n",
        "                y_scores = sub_df[target_var].values\n",
        "\n",
        "                fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "                roc_auc = auc(fpr, tpr)\n",
        "\n",
        "                if roc_auc < 0.5:\n",
        "                    y_scores = -y_scores\n",
        "                    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "                    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "                J = tpr - fpr\n",
        "                ix = np.argmax(J)\n",
        "                best_thresh = thresholds[ix]\n",
        "\n",
        "                y_pred = (y_scores >= best_thresh).astype(int)\n",
        "                tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "                sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "                label_text = (f\"{label} (AUC={roc_auc:.2f})\\n\"\n",
        "                              f\"Sens:{sens:.1%} Spec:{spec:.1%}\")\n",
        "\n",
        "                # === Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¬ÙˆÙ‡Ø±ÙŠ Ù‡Ù†Ø§ ===\n",
        "                # Ù†Ø¶Ø¹ Ø¹Ù„Ø§Ù…Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· ÙÙŠ Ù…Ù†ØªØµÙ Ø§Ù„Ø®Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… markevery Ù…Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ù…Ø¤Ø´Ø± ÙˆØ§Ø­Ø¯\n",
        "                mid_index = len(fpr) // 2\n",
        "\n",
        "                plt.plot(fpr, tpr,\n",
        "                         color=colors[i],\n",
        "                         linestyle=linestyles[i],\n",
        "                         marker=markers[i],\n",
        "                         markevery=[mid_index],      # Ø¹Ù„Ø§Ù…Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· ÙÙŠ Ø§Ù„Ù…Ù†ØªØµÙ\n",
        "                         markersize=10,              # Ø­Ø¬Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© ÙƒØ¨ÙŠØ± ÙˆÙˆØ§Ø¶Ø­\n",
        "                         lw=2.5,\n",
        "                         label=label_text)\n",
        "\n",
        "                # Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ù€ Cut-off Ø§Ù„Ø£ÙØ¶Ù„ (X Ø³ÙˆØ¯Ø§Ø¡)\n",
        "                plt.scatter(fpr[ix], tpr[ix], marker='X', color='black', s=100, zorder=10)\n",
        "\n",
        "            if plot_valid:\n",
        "                plt.plot([0, 1], [0, 1], color='gray', lw=1.5, linestyle=':')\n",
        "\n",
        "                plt.xlim([0.0, 1.0])\n",
        "                plt.ylim([0.0, 1.05])\n",
        "                plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12, fontweight='bold')\n",
        "                plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12, fontweight='bold')\n",
        "\n",
        "                plt.legend(loc=\"lower right\", fontsize=10, frameon=True, framealpha=0.9)\n",
        "                plt.grid(alpha=0.3)\n",
        "                plt.tight_layout()\n",
        "\n",
        "                safe_name = str(target_var).replace(\"/\", \"-\").replace(\"\\\\\", \"-\")\n",
        "                filename = f'ROC_{safe_name}.png'\n",
        "                plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "                plt.close()\n",
        "                print(f\"âœ… ØªÙ… Ø­ÙØ¸: {filename}\")\n",
        "            else:\n",
        "                plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {target_var}: {e}\")\n",
        "            plt.close()"
      ],
      "metadata": {
        "id": "APshC0FZGt5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Roc-Carve results**"
      ],
      "metadata": {
        "id": "MQNPVQJmOnID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "\n",
        "# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¯Ø§ØªØ§\n",
        "file_path = 'Data.xlsx'\n",
        "try:\n",
        "    df = pd.read_excel(file_path, sheet_name='Clean Data')\n",
        "except:\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "    except:\n",
        "        df = pd.DataFrame()\n",
        "\n",
        "if not df.empty:\n",
        "    print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.\")\n",
        "\n",
        "    # ØªÙ†Ø¸ÙŠÙ Ø§Ø³Ù… Ø§Ù„Ø¬Ø±ÙˆØ¨\n",
        "    if 'Group' in df.columns:\n",
        "        df['Group'] = df['Group'].astype(str).str.replace('\"', '').str.replace(\"'\", \"\").str.strip()\n",
        "\n",
        "    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©\n",
        "    exclude_cols = ['Group', 'Name', 'Sex', 'id', 'ID']\n",
        "    target_vars = [col for col in df.select_dtypes(include=[np.number]).columns if col not in exclude_cols]\n",
        "\n",
        "    # ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø§Øª (Ù†ÙØ³ Ø§Ù„Ù…Ø³Ù…ÙŠØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©)\n",
        "    comparisons = [\n",
        "        ('Group 1', 'Group 2', 'Control vs NDP'),\n",
        "        ('Group 1', 'Group 3', 'Control vs TP'),\n",
        "        ('Group 3', 'Group 2', 'TP vs NDP')\n",
        "    ]\n",
        "\n",
        "    # Ù‚Ø§Ø¦Ù…Ø© Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "    results_list = []\n",
        "\n",
        "    print(\"â³ Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØªØ¬Ù…ÙŠØ¹Ù‡Ø§...\")\n",
        "\n",
        "    for target_var in target_vars:\n",
        "        for neg_g, pos_g, label in comparisons:\n",
        "            # ÙÙ„ØªØ±Ø© Ø§Ù„Ø¯Ø§ØªØ§\n",
        "            sub_df = df[df['Group'].isin([neg_g, pos_g])].dropna(subset=[target_var])\n",
        "\n",
        "            if sub_df.empty or sub_df[target_var].nunique() < 2:\n",
        "                continue\n",
        "\n",
        "            # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù‚ÙŠÙ…\n",
        "            y_true = (sub_df['Group'] == pos_g).astype(int)\n",
        "            y_scores = sub_df[target_var].values\n",
        "\n",
        "            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù€ ROC\n",
        "            fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø§Ù„Ø¹ÙƒØ³ÙŠØ© ÙˆÙ‚Ù„Ø¨Ù‡Ø§ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±\n",
        "            is_inverted = False\n",
        "            if roc_auc < 0.5:\n",
        "                y_scores = -y_scores\n",
        "                fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "                roc_auc = auc(fpr, tpr)\n",
        "                is_inverted = True\n",
        "\n",
        "            # Ø­Ø³Ø§Ø¨ Ø£ÙØ¶Ù„ Ù†Ù‚Ø·Ø© (Optimal Threshold)\n",
        "            J = tpr - fpr\n",
        "            ix = np.argmax(J)\n",
        "            best_thresh = thresholds[ix]\n",
        "\n",
        "            # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ù„Ù€ Threshold Ù„Ùˆ ØªÙ… Ù‚Ù„Ø¨ Ø§Ù„ÙƒÙŠØ±Ù\n",
        "            display_thresh = -best_thresh if is_inverted else best_thresh\n",
        "\n",
        "            # Ø­Ø³Ø§Ø¨ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ (Confusion Matrix)\n",
        "            y_pred = (y_scores >= best_thresh).astype(int)\n",
        "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„Ø®ØµÙˆØµÙŠØ©\n",
        "            sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "            accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù„Ù‚Ø§Ø¦Ù…Ø©\n",
        "            results_list.append({\n",
        "                'Variable': target_var,\n",
        "                'Comparison': label,\n",
        "                'AUC': round(roc_auc, 3),\n",
        "                'Cut-off Value': round(display_thresh, 3),\n",
        "                'Sensitivity (%)': round(sens * 100, 1),\n",
        "                'Specificity (%)': round(spec * 100, 1),\n",
        "                'Accuracy (%)': round(accuracy * 100, 1),\n",
        "                'True Positive (TP)': tp,\n",
        "                'False Positive (FP)': fp,\n",
        "                'True Negative (TN)': tn,\n",
        "                'False Negative (FN)': fn\n",
        "            })\n",
        "\n",
        "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ù„Ù‰ DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "\n",
        "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø¥ÙƒØ³ÙŠÙ„\n",
        "    output_filename = 'ROC_Analysis_Results.xlsx'\n",
        "    results_df.to_excel(output_filename, index=False)\n",
        "\n",
        "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù: {output_filename}\")\n",
        "    print(results_df.head()) # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 Ø³Ø·ÙˆØ± Ù„Ù„ØªØ£ÙƒØ¯\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\")"
      ],
      "metadata": {
        "id": "hnKmHEm4UbzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Correlation Analysis HO-1, CP and CEA**"
      ],
      "metadata": {
        "id": "79P_sHH5dqpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import spearmanr\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø³Ù… (Ù…Ø³Ø§Ø¹Ø¯Ø©) ---\n",
        "def plot_spearman(ax, data, x_var, y_var, title, color):\n",
        "    valid_data = data[[x_var, y_var]].apply(pd.to_numeric, errors='coerce').dropna()\n",
        "\n",
        "    if len(valid_data) < 2:\n",
        "        ax.text(0.5, 0.5, \"Not enough data\", transform=ax.transAxes, ha='center', va='center')\n",
        "        # ØªÙ… Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù‡Ù†Ø§\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø§Ù…Ù„ Ø³Ø¨ÙŠØ±Ù…Ø§Ù†\n",
        "    r, p = spearmanr(valid_data[x_var], valid_data[y_var], nan_policy='omit')\n",
        "    p_text = \"p < 0.001\" if p < 0.001 else f\"p = {p:.3f}\"\n",
        "    stat_text = f\"Spearman r = {r:.3f}\\n{p_text}\"\n",
        "\n",
        "    # Ø§Ù„Ø±Ø³Ù…\n",
        "    sns.regplot(\n",
        "        data=valid_data, x=x_var, y=y_var, ax=ax,\n",
        "        scatter_kws={'alpha': 0.6, 'color': color, 's': 50},\n",
        "        line_kws={'color': 'black', 'linestyle': '--'}, ci=95\n",
        "    )\n",
        "\n",
        "    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª\n",
        "    ax.text(0.05, 0.95, stat_text, transform=ax.transAxes, va='top', ha='left',\n",
        "            fontsize=10, bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.7, ec='none'))\n",
        "\n",
        "    # --- ØªÙ… Ø¥Ø²Ø§Ù„Ø© Ø³Ø·Ø± Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ---\n",
        "    # ax.set_title(title, fontsize=12, weight='bold')\n",
        "\n",
        "    ax.set_xlabel(x_var, fontsize=11)\n",
        "    ax.set_ylabel(y_var, fontsize=11)\n",
        "    return r, p\n",
        "\n",
        "# --- Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ---\n",
        "try:\n",
        "    # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "    file_name = 'Data.xlsx'\n",
        "    sheet_name = 'Clean Data'\n",
        "\n",
        "    print(f\"Reading file: {file_name}, Sheet: {sheet_name}...\")\n",
        "    # ØªØ£ÙƒØ¯ Ù…Ù† Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµØ­ÙŠØ­ Ù„Ø¯ÙŠÙƒ\n",
        "    df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
        "\n",
        "    # 2. ØªÙ†Ø¸ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    # 3. ØªÙ†Ø¸ÙŠÙ Ø¹Ù…ÙˆØ¯ Group\n",
        "    df['Group'] = df['Group'].astype(str).str.replace('\"', '').str.strip()\n",
        "\n",
        "    group_map = {\n",
        "        'Group 1': 'Control',\n",
        "        'Group 2': 'Newly Diagnosed Patients',\n",
        "        'Group 3': 'Treated Patients'\n",
        "    }\n",
        "    df['Group'] = df['Group'].replace(group_map)\n",
        "\n",
        "    groups = ['Control', 'Newly Diagnosed Patients', 'Treated Patients']\n",
        "    colors = ['forestgreen', 'goldenrod', 'firebrick']\n",
        "\n",
        "    # 4. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙˆØ§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
        "    variables = ['HO-1', 'CP', 'CEA']\n",
        "    pairs = list(itertools.combinations(variables, 2)) # ÙŠÙ†Ø´Ø¦ ÙƒÙ„ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…Ù…ÙƒÙ†Ø©\n",
        "\n",
        "    sns.set(style='whitegrid')\n",
        "\n",
        "    # Ø§Ù„Ø¯ÙˆØ±Ø§Ù† Ø¹Ù„Ù‰ ÙƒÙ„ Ø²ÙˆØ¬ Ù…Ù† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙˆØ±Ø³Ù…Ù‡\n",
        "    for x_var, y_var in pairs:\n",
        "        print(f\"Processing: {x_var} vs {y_var}\")\n",
        "\n",
        "        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø£Ø±Ù‚Ø§Ù…\n",
        "        df[x_var] = pd.to_numeric(df[x_var], errors='coerce')\n",
        "        df[y_var] = pd.to_numeric(df[y_var], errors='coerce')\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "        # Ø±Ø³Ù… Ø§Ù„Ø¹ÙŠÙ†Ø© Ø§Ù„ÙƒÙ„ÙŠØ© (Total Sample)\n",
        "        plot_spearman(axes[0, 0], df, x_var, y_var, 'Total Sample', 'darkslateblue')\n",
        "\n",
        "        # Ø±Ø³Ù… Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ©\n",
        "        for i, group in enumerate(groups):\n",
        "            row = (i + 1) // 2\n",
        "            col = (i + 1) % 2\n",
        "            subset = df[df['Group'] == group]\n",
        "            plot_spearman(axes[row, col], subset, x_var, y_var, group, colors[i])\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "        # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø³Ù… Ø§Ù„Ù…ØªØºÙŠØ±ÙŠÙ†\n",
        "        file_out = f'spearman_{x_var}_{y_var}.png'.replace(' ', '_')\n",
        "        plt.savefig(file_out)\n",
        "        print(f\"Saved plot: {file_out}\")\n",
        "        plt.close() # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø±Ø³Ù… Ù„Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù„Ù„Ø²ÙˆØ¬ Ø§Ù„ØªØ§Ù„ÙŠ\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {e}\")"
      ],
      "metadata": {
        "id": "YHp1OUmwdjyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Heatmap (pearson and spearman)**"
      ],
      "metadata": {
        "id": "XwHq3A8fVhfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø±Ø³Ù… ---\n",
        "sns.set(style='white')\n",
        "\n",
        "# --- Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØµÙÙˆÙØ§Øª ---\n",
        "def calculate_corr_matrix(data, method='pearson'):\n",
        "    cols = data.columns\n",
        "    n = len(cols)\n",
        "\n",
        "    corr_matrix = pd.DataFrame(np.zeros((n, n)), index=cols, columns=cols)\n",
        "    annot_matrix = pd.DataFrame(np.empty((n, n), dtype=object), index=cols, columns=cols)\n",
        "    excel_matrix = pd.DataFrame(np.empty((n, n), dtype=object), index=cols, columns=cols)\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            c1 = cols[i]\n",
        "            c2 = cols[j]\n",
        "\n",
        "            if i == j:\n",
        "                corr_matrix.iloc[i, j] = 1.0\n",
        "                annot_matrix.iloc[i, j] = \"1.00\"\n",
        "                excel_matrix.iloc[i, j] = \"1.00\"\n",
        "                continue\n",
        "\n",
        "            valid_data = data[[c1, c2]].dropna()\n",
        "\n",
        "            if len(valid_data) < 2:\n",
        "                corr_matrix.iloc[i, j] = 0\n",
        "                annot_matrix.iloc[i, j] = \"NaN\"\n",
        "                excel_matrix.iloc[i, j] = \"NaN\"\n",
        "                continue\n",
        "\n",
        "            if method == 'pearson':\n",
        "                r, p = pearsonr(valid_data[c1], valid_data[c2])\n",
        "            else:\n",
        "                r, p = spearmanr(valid_data[c1], valid_data[c2])\n",
        "\n",
        "            corr_matrix.iloc[i, j] = r\n",
        "\n",
        "            if p < 0.001:\n",
        "                p_text = \"p<0.001\"\n",
        "            else:\n",
        "                p_text = f\"p={p:.3f}\"\n",
        "\n",
        "            annot_matrix.iloc[i, j] = f\"{r:.2f}\\n({p_text})\"\n",
        "            excel_matrix.iloc[i, j] = f\"{r:.3f} ({p_text})\"\n",
        "\n",
        "    return corr_matrix, annot_matrix, excel_matrix\n",
        "\n",
        "# --- Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ---\n",
        "try:\n",
        "    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "    file_name = 'Data.xlsx'\n",
        "    sheet_name = 'Clean Data'\n",
        "    df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
        "\n",
        "    df.columns = df.columns.str.strip()\n",
        "    numeric_cols = ['HO-1', 'CP', 'CEA', 'Hb', 'Creatinine', 'ALT', 'Age', 'Cu', 'Fe', 'Zn', 'Ca']\n",
        "    for col in numeric_cols:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df_numeric = df[numeric_cols]\n",
        "\n",
        "    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØµÙÙˆÙØ§Øª\n",
        "    r_mat_p, annot_mat_p, excel_mat_p = calculate_corr_matrix(df_numeric, method='pearson')\n",
        "    r_mat_s, annot_mat_s, excel_mat_s = calculate_corr_matrix(df_numeric, method='spearman')\n",
        "\n",
        "    unified_cmap = 'coolwarm'\n",
        "\n",
        "    # --- Ø±Ø³Ù… Pearson (Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†) ---\n",
        "    plt.figure(figsize=(16, 14))\n",
        "    sns.heatmap(r_mat_p, annot=annot_mat_p, fmt='', cmap=unified_cmap, center=0, linewidths=.5, cbar_kws={\"shrink\": .8}, annot_kws={\"size\": 9})\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('heatmap_pearson_no_title.png')\n",
        "    print(\"Saved: heatmap_pearson_no_title.png\")\n",
        "\n",
        "    # --- Ø±Ø³Ù… Spearman (Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†) ---\n",
        "    plt.figure(figsize=(16, 14))\n",
        "    sns.heatmap(r_mat_s, annot=annot_mat_s, fmt='', cmap=unified_cmap, center=0, linewidths=.5, cbar_kws={\"shrink\": .8}, annot_kws={\"size\": 9})\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('heatmap_spearman_no_title.png')\n",
        "    print(\"Saved: heatmap_spearman_no_title.png\")\n",
        "\n",
        "    # --- Ø­ÙØ¸ Ø§Ù„Ø¥ÙƒØ³ÙŠÙ„ ---\n",
        "    output_filename = 'correlation_analysis_results.xlsx'\n",
        "    with pd.ExcelWriter(output_filename) as writer:\n",
        "        excel_mat_p.to_excel(writer, sheet_name='Pearson Correlation')\n",
        "        excel_mat_s.to_excel(writer, sheet_name='Spearman Correlation')\n",
        "    print(f\"Saved Excel: {output_filename}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "HOaehEFKjTEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from google.colab import files\n",
        "import io\n",
        "import os\n",
        "\n",
        "print(\"--- Ultimate Paired T-Test Analysis (Publication Quality) ---\")\n",
        "print(\"Includes: Mean Â± SD, Mean Difference, 95% CI, t-value, P-value\")\n",
        "\n",
        "# 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„Ù\n",
        "target_file = 'matched group1&2.xlsx'\n",
        "\n",
        "if os.path.exists(target_file):\n",
        "    print(f\"âœ… Ø§Ù„Ù…Ù„Ù '{target_file}' Ù…ÙˆØ¬ÙˆØ¯. Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„...\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹Ù‡ Ø§Ù„Ø¢Ù†:\")\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        target_file = list(uploaded.keys())[0]\n",
        "    else:\n",
        "        target_file = None\n",
        "\n",
        "if target_file:\n",
        "    try:\n",
        "        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù\n",
        "        if target_file.endswith('.csv'):\n",
        "            df_raw = pd.read_csv(target_file, header=None)\n",
        "        else:\n",
        "            df_raw = pd.read_excel(target_file, header=None)\n",
        "\n",
        "        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª\n",
        "        group_rows = df_raw[df_raw[0].astype(str).str.contains(\"Group\", case=False, na=False)].index.tolist()\n",
        "\n",
        "        if len(group_rows) >= 2:\n",
        "            # === Ø§Ø³ØªØ®Ø±Ø§Ø¬ Newly Diagnosed (Before) ===\n",
        "            idx_g2_header = group_rows[0] + 1\n",
        "            idx_g2_end = group_rows[1]\n",
        "            df_before = df_raw.iloc[idx_g2_header+1 : idx_g2_end].copy()\n",
        "            df_before.columns = df_raw.iloc[idx_g2_header]\n",
        "            df_before = df_before.dropna(how='all')\n",
        "\n",
        "            # === Ø§Ø³ØªØ®Ø±Ø§Ø¬ Treated (After) ===\n",
        "            idx_g3_header = group_rows[1] + 1\n",
        "            df_after = df_raw.iloc[idx_g3_header+1 : ].copy()\n",
        "            df_after.columns = df_raw.iloc[idx_g3_header]\n",
        "            df_after = df_after.dropna(how='all')\n",
        "\n",
        "            # ØªØµØ­ÙŠØ­ Ø§Ù„Ø¥Ø²Ø§Ø­Ø© (Shift Fix)\n",
        "            if 'Name' in df_before.columns:\n",
        "                 if df_before['Name'].isnull().mean() > 0.5:\n",
        "                    col_idx_1 = df_before.columns[1]\n",
        "                    df_before['Name'] = df_before[col_idx_1]\n",
        "\n",
        "            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "            df_before['Name'] = df_before['Name'].astype(str).str.strip()\n",
        "            df_after['Name'] = df_after['Name'].astype(str).str.strip()\n",
        "            df_before.columns = df_before.columns.str.strip()\n",
        "            df_after.columns = df_after.columns.str.strip()\n",
        "\n",
        "            # Ø§Ù„Ø¯Ù…Ø¬ (Matching)\n",
        "            merged = pd.merge(df_before, df_after, on='Name', suffixes=('_Before', '_After'))\n",
        "\n",
        "            # Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª\n",
        "            exclude = ['Name', 'Sex', 'Group', 'id', 'ID', 'nan']\n",
        "            potential_cols = [c.replace('_Before', '') for c in merged.columns if '_Before' in c]\n",
        "            target_vars = [c for c in potential_cols if c not in exclude]\n",
        "\n",
        "            results = []\n",
        "            for var in target_vars:\n",
        "                col_pre = f\"{var}_Before\"\n",
        "                col_post = f\"{var}_After\"\n",
        "\n",
        "                # ØªØ­ÙˆÙŠÙ„ Ù„Ø£Ø±Ù‚Ø§Ù…\n",
        "                merged[col_pre] = pd.to_numeric(merged[col_pre], errors='coerce')\n",
        "                merged[col_post] = pd.to_numeric(merged[col_post], errors='coerce')\n",
        "\n",
        "                valid = merged[[col_pre, col_post]].dropna()\n",
        "\n",
        "                if len(valid) < 2: continue\n",
        "\n",
        "                # 1. Basic Stats\n",
        "                mean_pre = valid[col_pre].mean()\n",
        "                sd_pre = valid[col_pre].std()\n",
        "                mean_post = valid[col_post].mean()\n",
        "                sd_post = valid[col_post].std()\n",
        "\n",
        "                # 2. Difference Stats\n",
        "                diff = valid[col_pre] - valid[col_post]\n",
        "                mean_diff = diff.mean()\n",
        "                std_diff = diff.std()\n",
        "                n = len(valid)\n",
        "                se_diff = std_diff / np.sqrt(n)\n",
        "\n",
        "                # 3. T-Test & CI\n",
        "                if np.allclose(valid[col_pre], valid[col_post]):\n",
        "                    t_stat, p_val = 0, 1.0\n",
        "                    ci_lower, ci_upper = 0, 0\n",
        "                else:\n",
        "                    t_stat, p_val = stats.ttest_rel(valid[col_pre], valid[col_post])\n",
        "                    # Ø­Ø³Ø§Ø¨ ÙØªØ±Ø© Ø§Ù„Ø«Ù‚Ø© 95% Ù„Ù„ÙØ±Ù‚\n",
        "                    ci = stats.t.interval(0.95, df=n-1, loc=mean_diff, scale=se_diff)\n",
        "                    ci_lower, ci_upper = ci\n",
        "\n",
        "                # Significance Stars\n",
        "                sig = 'NS'\n",
        "                if p_val < 0.05: sig = '*'\n",
        "                if p_val < 0.01: sig = '**'\n",
        "                if p_val < 0.001: sig = '***'\n",
        "\n",
        "                results.append({\n",
        "                    'Variable': var,\n",
        "                    'Newly Diagnosed (Mean Â± SD)': f\"{mean_pre:.2f} Â± {sd_pre:.2f}\",\n",
        "                    'Treated (Mean Â± SD)': f\"{mean_post:.2f} Â± {sd_post:.2f}\",\n",
        "                    'Mean Diff.': f\"{mean_diff:.2f}\",  # Ø§Ù„ÙØ±Ù‚\n",
        "                    '95% CI of Diff.': f\"({ci_lower:.2f}, {ci_upper:.2f})\", # ÙØªØ±Ø© Ø§Ù„Ø«Ù‚Ø©\n",
        "                    't-statistic': f\"{t_stat:.3f}\",\n",
        "                    'P-value': \"< 0.001\" if p_val < 0.001 else f\"{p_val:.3f}\",\n",
        "                    'Sig.': sig\n",
        "                })\n",
        "\n",
        "            if results:\n",
        "                results_df = pd.DataFrame(results)\n",
        "\n",
        "                # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ\n",
        "                cols = [\n",
        "                    'Variable',\n",
        "                    'Newly Diagnosed (Mean Â± SD)',\n",
        "                    'Treated (Mean Â± SD)',\n",
        "                    'Mean Diff.',\n",
        "                    '95% CI of Diff.',\n",
        "                    't-statistic',\n",
        "                    'P-value',\n",
        "                    'Sig.'\n",
        "                ]\n",
        "                results_df = results_df[cols]\n",
        "\n",
        "                print(\"\\n=== Comprehensive Paired T-Test Report ===\")\n",
        "                display(results_df)\n",
        "\n",
        "                out_file = 'Paired_Ttest_Full_Report.xlsx'\n",
        "                results_df.to_excel(out_file, index=False)\n",
        "                print(f\"\\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„: {out_file}\")\n",
        "                files.download(out_file)\n",
        "            else:\n",
        "                print(\"âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {e}\")"
      ],
      "metadata": {
        "id": "2_Gx6_XgjVUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from google.colab import files\n",
        "import io\n",
        "import os\n",
        "\n",
        "print(\"--- Ultimate Wilcoxon Signed-Rank Test Analysis (Publication Quality) ---\")\n",
        "print(\"Includes: Median (IQR), Wilcoxon Statistic, P-value, Significance\")\n",
        "\n",
        "# 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù\n",
        "target_file = 'matched group1&2.xlsx' # Ø£Ùˆ .csv\n",
        "\n",
        "# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØŒ ÙˆØ¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ Ù†Ø·Ù„Ø¨ Ø§Ù„Ø±ÙØ¹\n",
        "if os.path.exists(target_file):\n",
        "    print(f\"âœ… Ø§Ù„Ù…Ù„Ù '{target_file}' Ù…ÙˆØ¬ÙˆØ¯. Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„...\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹Ù‡ Ø§Ù„Ø¢Ù†:\")\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        target_file = list(uploaded.keys())[0]\n",
        "    else:\n",
        "        target_file = None\n",
        "\n",
        "if target_file:\n",
        "    try:\n",
        "        # 2. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù\n",
        "        if target_file.endswith('.csv'):\n",
        "            df_raw = pd.read_csv(target_file, header=None)\n",
        "        else:\n",
        "            df_raw = pd.read_excel(target_file, header=None)\n",
        "\n",
        "        # 3. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª\n",
        "        group_rows = df_raw[df_raw[0].astype(str).str.contains(\"Group\", case=False, na=False)].index.tolist()\n",
        "\n",
        "        if len(group_rows) >= 2:\n",
        "            # === Ø§Ø³ØªØ®Ø±Ø§Ø¬ Newly Diagnosed (Before) ===\n",
        "            idx_g2_header = group_rows[0] + 1\n",
        "            idx_g2_end = group_rows[1]\n",
        "            df_before = df_raw.iloc[idx_g2_header+1 : idx_g2_end].copy()\n",
        "            df_before.columns = df_raw.iloc[idx_g2_header]\n",
        "            df_before = df_before.dropna(how='all')\n",
        "\n",
        "            # === Ø§Ø³ØªØ®Ø±Ø§Ø¬ Treated (After) ===\n",
        "            idx_g3_header = group_rows[1] + 1\n",
        "            df_after = df_raw.iloc[idx_g3_header+1 : ].copy()\n",
        "            df_after.columns = df_raw.iloc[idx_g3_header]\n",
        "            df_after = df_after.dropna(how='all')\n",
        "\n",
        "            # ØªØµØ­ÙŠØ­ Ø§Ù„Ø¥Ø²Ø§Ø­Ø© (Shift Fix) Ù„Ù„Ø£Ø³Ù…Ø§Ø¡\n",
        "            if 'Name' in df_before.columns:\n",
        "                 if df_before['Name'].isnull().mean() > 0.5:\n",
        "                    col_idx_1 = df_before.columns[1]\n",
        "                    df_before['Name'] = df_before[col_idx_1]\n",
        "\n",
        "            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "            df_before['Name'] = df_before['Name'].astype(str).str.strip()\n",
        "            df_after['Name'] = df_after['Name'].astype(str).str.strip()\n",
        "            df_before.columns = df_before.columns.str.strip()\n",
        "            df_after.columns = df_after.columns.str.strip()\n",
        "\n",
        "            # Ø§Ù„Ø¯Ù…Ø¬ (Matching)\n",
        "            merged = pd.merge(df_before, df_after, on='Name', suffixes=('_Before', '_After'))\n",
        "\n",
        "            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª\n",
        "            exclude = ['Name', 'Sex', 'Group', 'id', 'ID', 'nan']\n",
        "            potential_cols = [c.replace('_Before', '') for c in merged.columns if '_Before' in c]\n",
        "            target_vars = [c for c in potential_cols if c not in exclude]\n",
        "\n",
        "            results = []\n",
        "            for var in target_vars:\n",
        "                col_pre = f\"{var}_Before\"\n",
        "                col_post = f\"{var}_After\"\n",
        "\n",
        "                # ØªØ­ÙˆÙŠÙ„ Ù„Ø£Ø±Ù‚Ø§Ù…\n",
        "                merged[col_pre] = pd.to_numeric(merged[col_pre], errors='coerce')\n",
        "                merged[col_post] = pd.to_numeric(merged[col_post], errors='coerce')\n",
        "\n",
        "                valid = merged[[col_pre, col_post]].dropna()\n",
        "\n",
        "                if len(valid) < 2: continue\n",
        "\n",
        "                # 1. Ø­Ø³Ø§Ø¨ Median Ùˆ IQR (Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆÙŠÙ„ÙƒÙˆÙƒØ³ÙˆÙ†)\n",
        "                def get_median_iqr(series):\n",
        "                    med = series.median()\n",
        "                    q1 = series.quantile(0.25)\n",
        "                    q3 = series.quantile(0.75)\n",
        "                    iqr = q3 - q1\n",
        "                    return med, iqr\n",
        "\n",
        "                med_pre, iqr_pre = get_median_iqr(valid[col_pre])\n",
        "                med_post, iqr_post = get_median_iqr(valid[col_post])\n",
        "\n",
        "                # 2. Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Wilcoxon\n",
        "                # Ù†ØªØ­Ù‚Ù‚ Ø£ÙˆÙ„Ø§Ù‹ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‚ÙŠÙ… Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\n",
        "                diff = valid[col_pre] - valid[col_post]\n",
        "                if (diff == 0).all():\n",
        "                    stat, p_val = 0, 1.0\n",
        "                    sig = 'Identical'\n",
        "                else:\n",
        "                    try:\n",
        "                        stat, p_val = stats.wilcoxon(valid[col_pre], valid[col_post])\n",
        "                        # Ø§Ù„Ù†Ø¬ÙˆÙ… Ù„Ù„Ø¯Ù„Ø§Ù„Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©\n",
        "                        sig = 'NS'\n",
        "                        if p_val < 0.05: sig = '*'\n",
        "                        if p_val < 0.01: sig = '**'\n",
        "                        if p_val < 0.001: sig = '***'\n",
        "                    except Exception:\n",
        "                        stat, p_val = np.nan, np.nan\n",
        "                        sig = 'Error'\n",
        "\n",
        "                results.append({\n",
        "                    'Variable': var,\n",
        "                    'Newly Diagnosed (Median Â± IQR)': f\"{med_pre:.2f} Â± {iqr_pre:.2f}\",\n",
        "                    'Treated (Median Â± IQR)': f\"{med_post:.2f} Â± {iqr_post:.2f}\",\n",
        "                    'Wilcoxon Stat': f\"{stat:.1f}\",\n",
        "                    'P-value': \"< 0.001\" if p_val < 0.001 else f\"{p_val:.5f}\",\n",
        "                    'Sig.': sig\n",
        "                })\n",
        "\n",
        "            if results:\n",
        "                results_df = pd.DataFrame(results)\n",
        "\n",
        "                # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
        "                cols = [\n",
        "                    'Variable',\n",
        "                    'Newly Diagnosed (Median Â± IQR)',\n",
        "                    'Treated (Median Â± IQR)',\n",
        "                    'Wilcoxon Stat',\n",
        "                    'P-value',\n",
        "                    'Sig.'\n",
        "                ]\n",
        "                results_df = results_df[cols]\n",
        "\n",
        "                print(\"\\n=== Comprehensive Wilcoxon Test Report ===\")\n",
        "                try:\n",
        "                    from IPython.display import display\n",
        "                    display(results_df)\n",
        "                except:\n",
        "                    print(results_df)\n",
        "\n",
        "                # Ø­ÙØ¸ ÙˆØªÙ†Ø²ÙŠÙ„\n",
        "                out_file = 'Wilcoxon_Test_Full_Report.xlsx'\n",
        "                results_df.to_excel(out_file, index=False)\n",
        "                print(f\"\\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {out_file}\")\n",
        "                files.download(out_file)\n",
        "            else:\n",
        "                print(\"âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©.\")\n",
        "        else:\n",
        "             print(\"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª (Groups) ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ù…Ù„Ù.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {e}\")"
      ],
      "metadata": {
        "id": "h7kDRQdbDkaW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}